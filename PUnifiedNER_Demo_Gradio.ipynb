{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1eeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "from transformers import BertTokenizer,T5Tokenizer, T5ForConditionalGeneration, MT5ForConditionalGeneration, Text2TextGenerationPipeline\n",
    "import sys, os, torch\n",
    "import json\n",
    "sys.path.append(os.getcwd())\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bba96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('./models/my_t5_base/')\n",
    "model = MT5ForConditionalGeneration.from_pretrained('../v1.3/').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12da31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7860/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f18b769ed00>, 'http://localhost:7860/', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Blocks()\n",
    "\n",
    "\n",
    "def inference(text,tags):\n",
    "    prefixed_prompts = ''\n",
    "    for tag in tags:\n",
    "        prefixed_prompts = prefixed_prompts+f'<实体><{tag}>'\n",
    "        \n",
    "    print(tags)\n",
    "     \n",
    "    prefixed_prompts = prefixed_prompts+'<文本>'\n",
    "    print(prefixed_prompts)\n",
    "    \n",
    "    input_sequences = [prefixed_prompts+text]\n",
    "    input_sequences = [text.lower() for text in input_sequences]\n",
    "\n",
    "\n",
    "    encoding = tokenizer(input_sequences,padding=\"longest\", max_length=512, \\\n",
    "                         truncation=True, return_tensors=\"pt\",)\n",
    "    input_ids, attention_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n",
    "\n",
    "    # inference\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,\\\n",
    "                         do_sample=False,eos_token_id=tokenizer.sep_token_id,num_beams = 10, max_length= 512,\n",
    "                        decoder_start_token_id=tokenizer.cls_token_id)\n",
    "    \n",
    "    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    preds = ''.join(preds.split(' '))\n",
    "    preds = preds.split(',')\n",
    "    preds = [item.replace(')','').replace('(','').split(':') for item in preds]\n",
    "    preds = [item for item in preds if item[0] in tags]\n",
    "    print(preds)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "## format prediction results from list of tuples -> dictionary with offsets\n",
    "def add_offset(text, preds):\n",
    "   \n",
    "    lower_text = text.lower()\n",
    "    ner_dict = {}\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "\n",
    "        start = lower_text.find(pred[1])\n",
    "        if start==-1:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            tag = pred[0]\n",
    "            if tag not in ner_dict:\n",
    "                end = start+len(pred[1])\n",
    "                tag_text = text[start:end]\n",
    "                tag = pred[0]\n",
    "                ner_dict[tag] = {tag_text:[[start,end-1]]}\n",
    "            else:\n",
    "                end = start+len(pred[1])\n",
    "                tag_text = text[start:end]\n",
    "\n",
    "                if tag_text in ner_dict[tag]:\n",
    "                    old_end = ner_dict[tag][tag_text][-1][1]\n",
    "        #                     print(idx)\n",
    "                    new_start = lower_text[old_end:].find(pred[1])\n",
    "                    new_start = new_start+old_end\n",
    "                    new_end = new_start+len(pred[1])\n",
    "                    ner_dict[tag][tag_text].append([new_start,new_end-1])\n",
    "                else:\n",
    "                    ner_dict[tag].update({tag_text:[[start,end-1]]})\n",
    "    print(ner_dict)\n",
    "    return ner_dict\n",
    "\n",
    "## format prediction results from dictionary with offsets -> gradio highlighted text output [(word, tag)...(word,tag)]\n",
    "def dict_to_gradio_output(text, ner_dict):\n",
    "    \n",
    "    results = []\n",
    "    for tag,value in ner_dict.items():\n",
    "        for word, offsets in value.items():\n",
    "            for offset in offsets:\n",
    "                results.append([offset[0],offset[1],word,tag])   \n",
    "                \n",
    "    print(results)\n",
    "    results = sorted(results, key=lambda tup: tup[0])\n",
    "    print(results)\n",
    "            \n",
    "    newres = []\n",
    "    ntext = len(text)+1\n",
    "    for line in results:\n",
    "        if len(newres)==0:\n",
    "            if line[0]==0:\n",
    "                newres.append(line)\n",
    "            else:\n",
    "                begin = 0\n",
    "                end = line[0]\n",
    "                newres.append([begin,end-1,text[begin:end],None])\n",
    "                newres.append(line)\n",
    "        else:\n",
    "            if line[0]-newres[-1][1] > 1:\n",
    "                begin = newres[-1][1]+1\n",
    "                end = line[0]\n",
    "                newres.append([begin,end, text[begin:end], None])\n",
    "            newres.append(line)\n",
    "    if len(newres)==0:\n",
    "        return [(text,None)]\n",
    "    \n",
    "    if newres[-1][1] < ntext-1:\n",
    "        begin = newres[-1][1] + 1\n",
    "        end = ntext-1\n",
    "        newres.append([begin, end, text[begin:end], None])\n",
    "                \n",
    "    \n",
    "    gradio_result = []\n",
    "    for line in newres:\n",
    "        gradio_result.append((line[2],line[3]))\n",
    "            \n",
    "    print(gradio_result)\n",
    "    return gradio_result\n",
    "    \n",
    "    \n",
    "\n",
    "def entity_extraction(text,tags):\n",
    "\n",
    "    predictions = inference(text,tags)\n",
    "    ner_dict = add_offset(text,predictions)\n",
    "    final_output = dict_to_gradio_output(text,ner_dict)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "# inp = gr.Textbox(placeholder=\"请输入文本\", label=\"输入文本\")\n",
    "# inp1 = gr.Textbox(placeholder=\"请输入文本\", label=\"输入文本\")\n",
    "\n",
    "\n",
    "cluener_tags = ['名称','公司','游戏','组织','电影', '地点','职位','政府','景点','书籍']\n",
    "boson_tags = ['产品','时间', '名称', '组织', '地点','公司'] \n",
    "pd_tags = ['地点','名称','组织','时间']\n",
    "msra_tags = ['地点','名称','组织']\n",
    "ontonotes_tags = ['地缘政治实体','地点','名称','组织']\n",
    "chinese_address_tags = ['省份', '城市', '区', '街道', '社区','兴趣点', '路', '路号', '次兴趣点', '产业园', '楼号', '路口', '方位', '单元','楼层', '距离', '村组']\n",
    "resume_tags = ['名称','国籍', '民族', '职位', '学历', '公司', '专业', '籍贯']\n",
    "ecommerce_tags = ['品牌','商品']\n",
    "\n",
    "\n",
    "tags = ['品牌','商品', '地点','名称','组织','地缘政治实体','国籍', '民族', '职位', '学历', '公司', '专业', '籍贯','游戏','电影', '政府','景点','书籍',\n",
    "'产品','时间', '省份', '城市', '区', '街道', '社区','兴趣点', '路', '路号', '次兴趣点', '产业园', '楼号', '路口', '方位', '单元','楼层', '距离', '村组']\n",
    "\n",
    "extraction_iface = gr.Interface(\n",
    "    fn=entity_extraction,\n",
    "    inputs=['text',gr.CheckboxGroup(tags),],\n",
    "    outputs=gr.outputs.HighlightedText(label=\"命名实体识别\"),\n",
    "    examples=[['九城宣布将于美国东部时间2012年3月21日美国股市收盘后(北京时间3月22日)发布2011财年Q4及全年未审计财务报告',boson_tags],\n",
    "              [\"陕西省西安市碑林区东关南街街道仁厚南路高山流水星币传说0号楼\",chinese_address_tags],\n",
    "              ['百雀羚洗面奶 男士深层净化洁面泡沫150m 控油补水 收敛毛孔 官方',ecommerce_tags],\n",
    "              [\"怎样做才能避免拒签？新浪出国频道邀请到美国使馆签证处的签证官江德力（charles\",cluener_tags],\n",
    "              [\"致公党中央领导人多次参加中共中央和国务院举行的民主党派人士座谈会、协商会，参与国家大政方针的协商，认真履行参政议政、民主监督职能；\",msra_tags],\n",
    "              [\"1928年3月,土肥原出任张作霖的顾问,后一手策划了“皇姑屯事件”。1931年在天津设立特务机关,土肥原任机关长,后将溥仪从天津诱至大连,拼凑伪满傀儡政权。之后,土肥原被调往哈尔滨出任特务机关长,镇压东北抗日武装力量。\",pd_tags],\n",
    "              [\"1955年出生，兰州大学经济系硕士毕业，浙江大学经济学教授、博士生导师。\",resume_tags],\n",
    "              [\"报警人称其丈夫（小明，18112345678，47周岁，12345678123456781X，有精神病（医院确诊过）五分钟前醉酒驾车从金桥路杨高南路出去，车辆信息：浙AH057Y，银灰色起亚轿车，请各单位注意发现，谢谢配合！\",['名称','地点','公司']],\n",
    "              [\"黑色衬衣，蓝色牛仔裤的男子，2022年5月31日早上9点半左右，深圳湾科技园12A\",['时间','地点']],\n",
    "                ]\n",
    ")\n",
    "\n",
    "extraction_iface.launch(server_name=\"0.0.0.0\")\n",
    "\n",
    "# launcher = gr.Parallel(\n",
    "#             extraction_iface,\n",
    "#             examples=[\n",
    "#                 \"黑色衬衣，蓝色牛仔裤的男子，2022年5月31日早上9点半左右，深圳湾科技园12A\",\n",
    "#                 \"穿白色上衣的美团骑手 戴墨镜\",\n",
    "#                 \"补充 车牌，沪HN7892 蓝色出租车，沿着往长宁路往西逃逸 民警处罚违章车辆时被一辆疑似海博 蓝色出租车，尾号7862 带倒拖行，现民警受伤\",\n",
    "#                 \"报警人称其丈夫（小明，18112345678，47周岁，12345678123456781X，有精神病（医院确诊过）五分钟前醉酒驾车从金桥路杨高南路出去，车辆信息：浙AH057Y，银灰色起亚轿车，请各单位注意发现，谢谢配合！\",\n",
    "#                 \"一男子买了菜刀上了 沪GU2976 强生出租车 黑灰色衣服， 1米8左右，40岁左右。请你局工作中注意发现。\"\n",
    "#                 ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'省份', '城市', '区', '街道', '社区','兴趣点', '路', '路号', '次兴趣点', '产业园', '楼号', '路口', '方位', '单元','楼层', '距离', '村组'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62525b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5e8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e510a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['品牌','商品' '地点','名称','组织','地缘政治实体','国籍', '民族', '职位', '学历', '公司', '专业', '籍贯', '公司','游戏','电影', '政府','景点','书籍',\n",
    "'产品','时间', '省份', '城市', '区', '街道', '社区','兴趣点', '路', '路号', '次兴趣点', '产业园', '楼号', '路口', '方位', '单元','楼层', '距离', '村组']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f04699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "sorted_by_second = sorted(data, key=lambda tup: tup[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52744a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
